{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model Construction using Spark ML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To allocate more memory if necessary \n",
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper thread in order to have a stream running in the background in Jupyter\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "textFile = sc.textFile(\"./myoutput_historical_data_full\")\n",
    "textFile.count()\n",
    "\n",
    "print(\"safe edits count: \", textFile.filter(lambda line: '\"label\": \"safe\"' in line).count())\n",
    "print(\"unsafe edits count: \", textFile.filter(lambda line: '\"label\": \"unsafe\"' in line).count())\n",
    "print(\"vandal edits count: \", textFile.filter(lambda line: '\"label\": \"vandal\"' in line).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.046287367405979\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            text_new|            text_old|\n",
      "+-----+--------------------+--------------------+\n",
      "| safe|\n",
      "\n",
      "{{Infobox music...|\n",
      "\n",
      "{{Infobox music...|\n",
      "| safe|\n",
      "\n",
      "{{Infobox organ...|\n",
      "\n",
      "{{Infobox organ...|\n",
      "| safe|\n",
      "\n",
      "{{refimprove|da...|\n",
      "\n",
      "{{refimprove|da...|\n",
      "| safe| A relative  pron...| A relative  pron...|\n",
      "| safe|#REDIRECT [[Forei...|#REDIRECT [[Forei...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            text_new|            text_old|\n",
      "+-----+--------------------+--------------------+\n",
      "| safe|\n",
      "\n",
      "\n",
      "\n",
      "'''John Russe...|\n",
      "\n",
      "\n",
      "\n",
      "'''John Russe...|\n",
      "| safe|\n",
      "\n",
      "'''Roomy Pak'''...|\n",
      "\n",
      "'''Roomy Pak'''...|\n",
      "| safe|\n",
      "\n",
      "{{Infobox river...|{{Coord|59|19|48|...|\n",
      "| safe|#REDIRECT [[Econo...|#REDIRECT [[Econo...|\n",
      "| safe|'''Dao xiao mian'...|'''Dao xiao mian'...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in saved dataset\n",
    "\n",
    "df = spark.read.json(\"./myoutput_historical_data\") #full training data is myoutput_historical_data_full\n",
    "keep_list = ['label', 'text_new', 'text_old']\n",
    "df = df.select([column for column in df.columns if column in keep_list])\n",
    "df.dropna() # remove records with missing value in any column\n",
    "\n",
    "# Split into training and testing data\n",
    "df_train, df_test = df.randomSplit([0.8, 0.2],seed=15)\n",
    "training_ratio = df_train.count()/df_test.count()\n",
    "print(training_ratio) # check that training set has around 80% of records \n",
    "df_train.show(5)\n",
    "df_test.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+---------+\n",
      "|label|            text_new|            text_old|label_idx|\n",
      "+-----+--------------------+--------------------+---------+\n",
      "| safe|\n",
      "\n",
      "{{Infobox music...|\n",
      "\n",
      "{{Infobox music...|      0.0|\n",
      "| safe|\n",
      "\n",
      "{{Infobox organ...|\n",
      "\n",
      "{{Infobox organ...|      0.0|\n",
      "| safe|\n",
      "\n",
      "{{refimprove|da...|\n",
      "\n",
      "{{refimprove|da...|      0.0|\n",
      "| safe| A relative  pron...| A relative  pron...|      0.0|\n",
      "| safe|#REDIRECT [[Forei...|#REDIRECT [[Forei...|      0.0|\n",
      "+-----+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+---------+\n",
      "|label|            text_new|            text_old|label_idx|\n",
      "+-----+--------------------+--------------------+---------+\n",
      "| safe|\n",
      "\n",
      "\n",
      "\n",
      "'''John Russe...|\n",
      "\n",
      "\n",
      "\n",
      "'''John Russe...|      0.0|\n",
      "| safe|\n",
      "\n",
      "'''Roomy Pak'''...|\n",
      "\n",
      "'''Roomy Pak'''...|      0.0|\n",
      "| safe|\n",
      "\n",
      "{{Infobox river...|{{Coord|59|19|48|...|      0.0|\n",
      "| safe|#REDIRECT [[Econo...|#REDIRECT [[Econo...|      0.0|\n",
      "| safe|'''Dao xiao mian'...|'''Dao xiao mian'...|      0.0|\n",
      "+-----+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert string label to integer label \n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\n",
    "df_train = indexer.fit(df_train).transform(df_train) \n",
    "#df_train = df_train.withColumn(\"label_idx\",df_train[\"label_idx\"]).cast(IntegerType()))\n",
    "\n",
    "df_test = indexer.fit(df_test).transform(df_test) \n",
    "#df_test = df_test.withColumn(\"label_idx\",df_test[\"label_idx\"]).cast(IntegerType()))\n",
    "\n",
    "df_train.show(5)\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           terms_new|           terms_old|\n",
      "+--------------------+--------------------+\n",
      "|[{{infobox, music...|[{{infobox, music...|\n",
      "|[{{infobox, organ...|[{{infobox, organ...|\n",
      "|[{{refimprove|dat...|[{{refimprove|dat...|\n",
      "|[relative, pronou...|[relative, pronou...|\n",
      "|[#redirect, [[for...|[#redirect, [[for...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|           terms_new|           terms_old|\n",
      "+--------------------+--------------------+\n",
      "|['''john, russell...|['''john, russell...|\n",
      "|['''roomy, pak'''...|['''roomy, pak'''...|\n",
      "|[{{infobox, river...|[{{coord|59|19|48...|\n",
      "|[#redirect, [[eco...|[#redirect, [[eco...|\n",
      "|['''dao, xiao, mi...|['''dao, xiao, mi...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text and find difference \n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "\n",
    "# Words only \n",
    "tokenizer1 = RegexTokenizer(inputCol='text_new', outputCol='words_new', pattern='\\\\s+') \n",
    "tokenizer2 = RegexTokenizer(inputCol='text_old', outputCol='words_old', pattern='\\\\s+') #\\\\W\n",
    "\n",
    "# Remove stopwords \n",
    "remover1 = StopWordsRemover(inputCol='words_new', outputCol='terms_new')\n",
    "remover2 = StopWordsRemover(inputCol='words_old', outputCol='terms_old')\n",
    "\n",
    "df_train = tokenizer1.transform(df_train)\n",
    "df_train = tokenizer2.transform(df_train)\n",
    "df_train = remover1.transform(df_train)\n",
    "df_train = remover2.transform(df_train)\n",
    "\n",
    "df_test = tokenizer1.transform(df_test)\n",
    "df_test = tokenizer2.transform(df_test)\n",
    "df_test = remover1.transform(df_test)\n",
    "df_test = remover2.transform(df_test)\n",
    "\n",
    "df_train.select('terms_new','terms_old').show(5)\n",
    "df_test.select('terms_new','terms_old').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('label', 'string'),\n",
       " ('text_new', 'string'),\n",
       " ('text_old', 'string'),\n",
       " ('label_idx', 'double'),\n",
       " ('words_new', 'array<string>'),\n",
       " ('words_old', 'array<string>'),\n",
       " ('terms_new', 'array<string>'),\n",
       " ('terms_old', 'array<string>')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at data types\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                diff|\n",
      "+-----+--------------------+\n",
      "| safe|      [|first=david]|\n",
      "| safe|          ['k-word']|\n",
      "| safe|[magazine—the, we...|\n",
      "| safe|                  []|\n",
      "| safe|    [subpage}}, {{r]|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|                diff|\n",
      "+-----+--------------------+\n",
      "| safe|[estate,, estate,...|\n",
      "| safe|  [university|huron]|\n",
      "| safe|[eng.svg, mouth_c...|\n",
      "| safe|    [subpage}}, {{r]|\n",
      "| safe|[2020}}, stub|dat...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find difference between terms new and old \n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "differencer=udf(lambda x,y: list(set(x)-set(y)), ArrayType(StringType()))\n",
    "\n",
    "df_train=df_train.withColumn('diff', differencer('terms_new', 'terms_old'))\n",
    "df_test=df_test.withColumn('diff', differencer('terms_new', 'terms_old'))\n",
    "\n",
    "# drop unnecessary columns here\n",
    "keep_list = ['label', 'label_idx', 'diff']\n",
    "df_train = df_train.select([column for column in df_train.columns if column in keep_list])\n",
    "df_test = df_test.select([column for column in df_test.columns if column in keep_list])\n",
    "\n",
    "df_train.select('label','diff').show(5)\n",
    "df_test.select('label','diff').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HashingTF_171261aecd03, IDF_9ca52d516593, LogisticRegressionModel: uid = LogisticRegression_37b7305902a7, numClasses = 3, numFeatures = 1024]\n",
      "{}\n",
      "+------+---------+----------+-----+\n",
      "| label|label_idx|prediction|count|\n",
      "+------+---------+----------+-----+\n",
      "|vandal|      2.0|       0.0|   54|\n",
      "|unsafe|      1.0|       1.0|   17|\n",
      "|vandal|      2.0|       1.0|    2|\n",
      "|unsafe|      1.0|       0.0|  749|\n",
      "|  safe|      0.0|       0.0| 4340|\n",
      "|  safe|      0.0|       1.0|   23|\n",
      "+------+---------+----------+-----+\n",
      "\n",
      "Average AUC for the best model: 0.7751907484283969\n",
      "AUC for best model on testing data: 0.7745652350934468\n",
      "{Param(parent='HashingTF_171261aecd03', name='binary', doc='If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. Default False.'): False, Param(parent='HashingTF_171261aecd03', name='numFeatures', doc='number of features.'): 1024, Param(parent='HashingTF_171261aecd03', name='outputCol', doc='output column name.'): 'hash', Param(parent='HashingTF_171261aecd03', name='inputCol', doc='input column name.'): 'diff'}\n",
      "{Param(parent='IDF_9ca52d516593', name='minDocFreq', doc='minimum number of documents in which a term should appear for filtering (>= 0)'): 0, Param(parent='IDF_9ca52d516593', name='outputCol', doc='output column name'): 'features', Param(parent='IDF_9ca52d516593', name='inputCol', doc='input column name'): 'hash'}\n",
      "{Param(parent='LogisticRegression_37b7305902a7', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LogisticRegression_37b7305902a7', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0, Param(parent='LogisticRegression_37b7305902a7', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto', Param(parent='LogisticRegression_37b7305902a7', name='featuresCol', doc='features column name'): 'features', Param(parent='LogisticRegression_37b7305902a7', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LogisticRegression_37b7305902a7', name='labelCol', doc='label column name'): 'label_idx', Param(parent='LogisticRegression_37b7305902a7', name='maxIter', doc='maximum number of iterations (>= 0)'): 10, Param(parent='LogisticRegression_37b7305902a7', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='LogisticRegression_37b7305902a7', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability', Param(parent='LogisticRegression_37b7305902a7', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='LogisticRegression_37b7305902a7', name='regParam', doc='regularization parameter (>= 0)'): 0.3, Param(parent='LogisticRegression_37b7305902a7', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LogisticRegression_37b7305902a7', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5, Param(parent='LogisticRegression_37b7305902a7', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "# Construct Pipeline and pass in diff (difference between new and old text)  \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Convert diff to Hash to TF-IDF\n",
    "hasher = HashingTF(inputCol='diff', outputCol='hash')\n",
    "idf = IDF(inputCol=hasher.getOutputCol(), outputCol='features')\n",
    "\n",
    "# We try both Logistic regression and/or Naive Bayes\n",
    "ml_model= LogisticRegression(featuresCol = 'features', labelCol = 'label_idx')\n",
    "# ml_model= NaiveBayes(featuresCol = 'features', labelCol = 'label_idx')\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[hasher, idf, ml_model])\n",
    "\n",
    "# Create a multiclass classification evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'label_idx')\n",
    "\n",
    "# Create and build param grid \n",
    "# logistic regression params\n",
    "params = ParamGridBuilder() \\\n",
    "    .addGrid(hasher.numFeatures, [1024]) \\\n",
    "    .addGrid(ml_model.regParam, [0.3]) \\\n",
    "    .addGrid(ml_model.maxIter,[10]) \\\n",
    "    .build()\n",
    "# naive bayes params \n",
    "# params = ParamGridBuilder() \\\n",
    "#     .addGrid(hasher.numFeatures, [50]) \\\n",
    "#     .addGrid(ml_model.smoothing, [1.0]) \\\n",
    "#     .build()\n",
    "\n",
    "\n",
    "# Create cross-validator \n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=params, \n",
    "                    evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# Train model on multiple folds of the training data\n",
    "cv = cv.fit(df_train)\n",
    "\n",
    "# Get best model \n",
    "best_model = cv.bestModel\n",
    "\n",
    "# Look at the stages in the best model\n",
    "print(best_model.stages)\n",
    "\n",
    "# Get the parameters for the logistic regression object in the best model\n",
    "print(best_model.extractParamMap())\n",
    "\n",
    "# Generate predictions on testing data using the best model then calculate RMSE\n",
    "prediction = best_model.transform(df_test)\n",
    "evaluator.evaluate(prediction)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label','label_idx','prediction').count().show()\n",
    "\n",
    "# Average AUC for each parameter combination in grid\n",
    "avg_auc = cv.avgMetrics\n",
    "#print(\"Average AUC for each parameter combination in grid: \" + str(avg_auc))\n",
    "\n",
    "# Average AUC for the best model\n",
    "best_model_auc = max(cv.avgMetrics)\n",
    "print(\"Average AUC for the best model: \" + str(best_model_auc))\n",
    "\n",
    "# AUC for best model on testing data\n",
    "best_auc = evaluator.evaluate(cv.transform(df_test))\n",
    "print(\"AUC for best model on testing data: \" + str(best_auc))\n",
    "\n",
    "# Print out parameters for best model \n",
    "print(best_model.stages[0].extractParamMap()) # parameters for haser\n",
    "print(best_model.stages[1].extractParamMap()) # parameters for idf\n",
    "print(best_model.stages[2].extractParamMap()) # parameters for logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model \n",
    "modelPath = \"./models/lrmodel\"\n",
    "best_model.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
