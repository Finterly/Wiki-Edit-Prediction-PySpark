{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real time Spark Streaming Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class StreamingThread(Thread):\n",
    "    def __init__(self, ssc):\n",
    "        Thread.__init__(self)\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        ssc.start()\n",
    "        ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.10.122:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.10.122:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x113d84690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Stream, need to prepare the data for prediction\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# Enter model path\n",
    "modelPath = \"./models/lrmodel\" #or nbmodel Naive Bayes Model\n",
    "globals()['models_loaded'] = False\n",
    "\n",
    "def process(time, rdd):\n",
    "\n",
    "    if rdd.isEmpty():\n",
    "            return  \n",
    "    print(\"========= %s =========\" % str(time))  \n",
    "    \n",
    "    # Convert to data frame and print rdd \n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "\n",
    "    # Convert string label to integer label \n",
    "    indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_idx\")\n",
    "    label_indexer = indexer.fit(df)\n",
    "    df = label_indexer.transform(df) \n",
    "    \n",
    "    # Tokenize text, remove stopwords, and find difference \n",
    "    tokenizer1 = RegexTokenizer(inputCol='text_new', outputCol='words_new', pattern='\\\\s+') \n",
    "    tokenizer2 = RegexTokenizer(inputCol='text_old', outputCol='words_old', pattern='\\\\s+') #\\\\W\n",
    "    remover1 = StopWordsRemover(inputCol='words_new', outputCol='terms_new')\n",
    "    remover2 = StopWordsRemover(inputCol='words_old', outputCol='terms_old')\n",
    "    df = tokenizer1.transform(df)\n",
    "    df = tokenizer2.transform(df)\n",
    "    df = remover1.transform(df)\n",
    "    df = remover2.transform(df)\n",
    "    differencer=udf(lambda x,y: list(set(x)-set(y)), ArrayType(StringType()))\n",
    "    df = df.withColumn('diff', differencer('terms_new', 'terms_old'))\n",
    "    \n",
    "    # Load the model if not already loaded \n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load(modelPath)\n",
    "        globals()['models_loaded'] = True\n",
    "\n",
    "    # Predict using loaded model  \n",
    "    df_result = globals()['my_model'].transform(df)   \n",
    "    \n",
    "    # Retrieve label from index and print prediction results \n",
    "    converter = IndexToString(inputCol=\"prediction\", outputCol=\"predLabel\", labels=label_indexer.labels)\n",
    "    df_result = converter.transform(df_result)\n",
    "    df_result.select('label', 'diff', 'features', 'rawPrediction', 'probability', 'predLabel').show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start() # may take some time to start up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n",
      "========= 2020-05-18 14:12:00 =========\n",
      "+-------+------+------------+--------------------+--------------------+-----------+--------------------+\n",
      "|comment| label|   name_user|            text_new|            text_old| title_page|            url_page|\n",
      "+-------+------+------------+--------------------+--------------------+-----------+--------------------+\n",
      "|       |unsafe|46.7.158.182|{{Automatic taxob...|{{Automatic taxob...|Potter wasp|//en.wikipedia.or...|\n",
      "+-------+------+------------+--------------------+--------------------+-----------+--------------------+\n",
      "\n",
      "+------+--------------+--------------------+--------------------+--------------------+---------+\n",
      "| label|          diff|            features|       rawPrediction|         probability|predLabel|\n",
      "+------+--------------+--------------------+--------------------+--------------------+---------+\n",
      "|unsafe|[ass, '''lick]|(1024,[172,834],[...|[2.04575195654470...|[0.83669357967945...|   unsafe|\n",
      "+------+--------------+--------------------+--------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
